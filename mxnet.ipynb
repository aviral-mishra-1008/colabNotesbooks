{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwcT34gTBP8eNMQmV1KGKQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wD3sgxmZzjVh"},"outputs":[],"source":["#download Casia WebFace  REPLACE WITH IMPORTS FROM DATASET DIRECTORY\n","import gdown\n","file_id = '1KxNCrXzln0lal3N4JiYl9cFOIhT78y1l'\n","gdown.download(f'https://drive.google.com/uc?id={file_id}', quiet=False)"]},{"cell_type":"code","source":["!pip install mxnet"],"metadata":{"id":"hvxxA8QvzpUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Mention: NOW WE NEED TO LOCATE THE FILES OF MXNET LIBRARY AND MANUALLY CHANGE THE np.bool to bool to make things work\n","#File location\n","..python3.10/dist-packages/mxnet/numpy/utils.py"],"metadata":{"id":"Zj8BqfgUzsAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","\n","with zipfile.ZipFile('/content/faces_webface_112x112.zip', 'r') as zip_ref:\n","  zip_ref.extractall('/content/Casia')"],"metadata":{"id":"L-qcREc-z0Xd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import keras\n","import pickle\n","import tensorflow_hub as hub\n","import random\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","import cv2\n","\n","# !rm -rf /content/.cache && rm -rf /tmp/*\n","# from google.colab import drive\n","\n","# drive.mount('/content/drive')\n","\n","# !cp /content/drive/MyDrive/FaceData/data_test.pkl /content\n","# !cp /content/drive/MyDrive/FaceData/data_train.pkl /content\n","\n","# drive.flush_and_unmount()\n","\n","# with open('data_test.pkl', 'rb') as f:\n","#     test_df = pickle.load(f)\n","\n","# with open('data_train.pkl', 'rb') as f:\n","#     train_df = pickle.load(f)\n","\n","# x_train = train_df['image']\n","# y_train = train_df['person']\n","\n","# x_test = test_df['image']\n","# y_test = test_df['person']\n","\n","# def remove_extra_dim(image):\n","#     try:\n","#         return np.squeeze(image, axis=0)  # Efficient removal if size 1\n","#     except ValueError:\n","#         # Extra dimension not found, return the original data\n","#         print('failed')\n","#         return image\n","\n","# Loop through each image in the Series\n","# for index, image in x_train.items():  # Access elements using items()\n","#     x_train.loc[index] = remove_extra_dim(image)  # Modify Series in-place\n","\n","# for index, image in x_test.items():  # Access elements using items()\n","#     x_test.loc[index] = remove_extra_dim(image)  # Modify Series in-place\n","\n","# x_train[0].shape\n","\n","# x_train.shape\n","\n","# classes = set()\n","# for i in y_train:\n","#   classes.add(i)\n","\n","# classes = list(classes)\n","# classes\n","\n","def data_generator(batch_size=120):\n","\n","  dataDIR = '/content/Casia/faces_webface_112x112/'\n","  rec_file = dataDIR+'train.rec'\n","  idx_file = dataDIR+'train.idx'\n","\n","  with open('face_data_mapping.pkl','rb') as f:\n","    label_wise_sample = pickle.load(f)\n","\n","  while True:\n","    a = []\n","    p = []\n","    n = []\n","    for _ in range(batch_size):\n","      found = True\n","      while(found):\n","        pos_neg = random.sample(list(label_wise_sample.keys()), 2)\n","        try:\n","          positive_samples = random.sample((label_wise_sample[pos_neg [0]]), 2)\n","          negative_sample= random.sample((label_wise_sample[pos_neg [1]]),1)\n","          found = False\n","        except:\n","          continue\n","\n","      record = mx.recordio.MXIndexedRecordIO(idx_file, rec_file, 'r')\n","      label_wise_index = dict()\n","      a_img_ind = positive_samples[0]\n","      p_img_ind = positive_samples[1]\n","      n_img_ind = negative_sample[0]\n","\n","      images = []\n","\n","      image_record = record.read_idx(a_img_ind)\n","      headerA, imageA = mx.recordio.unpack_img(image_record)\n","\n","      images.append(imageA)\n","\n","      image_record = record.read_idx(p_img_ind)\n","      headerP, imageP = mx.recordio.unpack_img(image_record)\n","\n","      images.append(imageP)\n","\n","\n","      image_record = record.read_idx(n_img_ind)\n","      headerN, imageN = mx.recordio.unpack_img(image_record)\n","\n","      images.append(imageN)\n","\n","      processedImages = []\n","\n","      for i in images:\n","        image = i[:,:,::-1]\n","        image = cv2.resize(image, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n","        image = np.asarray(image)\n","        image = image.astype('float32') / 255.0\n","        processedImages.append(image)\n","\n","\n","      a.append(processedImages[0])\n","      p.append(processedImages[1])\n","      n.append(processedImages[2])\n","\n","    yield ([np.array(a), np.array(p), np.array(n)], np.zeros((batch_size, 1)).astype(\"float32\"))\n"],"metadata":{"id":"Z2m9ZAr20CoM"},"execution_count":null,"outputs":[]}]}