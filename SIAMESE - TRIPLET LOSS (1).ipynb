{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7706894,"sourceType":"datasetVersion","datasetId":4499626},{"sourceId":7728376,"sourceType":"datasetVersion","datasetId":4515546},{"sourceId":7731368,"sourceType":"datasetVersion","datasetId":4517777}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport keras \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport os\nimport sys\nimport tensorflow.keras.backend as K\nimport random\nimport os                      \nimport numpy as np              \nimport pandas as pd             \nimport torch             \nfrom PIL import Image, ImageOps, ImageFilter\nimport matplotlib.pyplot as plt \nimport torch.nn as nn           \nfrom torch.utils.data import DataLoader  \nfrom PIL import Image          \nimport torch.nn.functional as F \nimport torchvision.transforms as transforms   \nfrom torchvision.utils import make_grid    \nfrom torchvision.datasets import ImageFolder  \nfrom torch.utils.data import random_split\nimport time\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.regularizers import l2\n# from keras import backend as K\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.layers import Lambda\nimport numpy.random as rng\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-20T11:58:48.340907Z","iopub.execute_input":"2024-03-20T11:58:48.341333Z","iopub.status.idle":"2024-03-20T11:58:48.354634Z","shell.execute_reply.started":"2024-03-20T11:58:48.341295Z","shell.execute_reply":"2024-03-20T11:58:48.353672Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing import image\nfrom keras.applications.imagenet_utils import decode_predictions, preprocess_input\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, Dropout","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:48.356171Z","iopub.execute_input":"2024-03-20T11:58:48.356445Z","iopub.status.idle":"2024-03-20T11:58:48.367618Z","shell.execute_reply.started":"2024-03-20T11:58:48.356423Z","shell.execute_reply":"2024-03-20T11:58:48.366846Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nbase_model = load_model('/kaggle/input/modelh5/EFNetB1-ft-best.h5')\nbase_model.trainable = True","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:48.368878Z","iopub.execute_input":"2024-03-20T11:58:48.369343Z","iopub.status.idle":"2024-03-20T11:58:52.875124Z","shell.execute_reply.started":"2024-03-20T11:58:48.369310Z","shell.execute_reply":"2024-03-20T11:58:52.874168Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"base_model.input_shape","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.877188Z","iopub.execute_input":"2024-03-20T11:58:52.877496Z","iopub.status.idle":"2024-03-20T11:58:52.883986Z","shell.execute_reply.started":"2024-03-20T11:58:52.877469Z","shell.execute_reply":"2024-03-20T11:58:52.882934Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(None, 240, 240, 3)"},"metadata":{}}]},{"cell_type":"code","source":"base_model = Model(inputs=base_model.input, outputs=base_model.get_layer(\"avg_pool\").output)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.885176Z","iopub.execute_input":"2024-03-20T11:58:52.885454Z","iopub.status.idle":"2024-03-20T11:58:52.924704Z","shell.execute_reply.started":"2024-03-20T11:58:52.885430Z","shell.execute_reply":"2024-03-20T11:58:52.923863Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"base_model.output_shape","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.926612Z","iopub.execute_input":"2024-03-20T11:58:52.927007Z","iopub.status.idle":"2024-03-20T11:58:52.932660Z","shell.execute_reply.started":"2024-03-20T11:58:52.926981Z","shell.execute_reply":"2024-03-20T11:58:52.931672Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(None, 1280)"},"metadata":{}}]},{"cell_type":"code","source":"embeddings = base_model","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.933764Z","iopub.execute_input":"2024-03-20T11:58:52.934069Z","iopub.status.idle":"2024-03-20T11:58:52.940680Z","shell.execute_reply.started":"2024-03-20T11:58:52.934038Z","shell.execute_reply":"2024-03-20T11:58:52.939895Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"main_dir = \"/kaggle/input/rfcdata/reflection-connection-train-data/train\"","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.941928Z","iopub.execute_input":"2024-03-20T11:58:52.942210Z","iopub.status.idle":"2024-03-20T11:58:52.950192Z","shell.execute_reply.started":"2024-03-20T11:58:52.942188Z","shell.execute_reply":"2024-03-20T11:58:52.949398Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class MapFunction():\n    def __init__(self, imageSize):\n        self.imageSize = imageSize\n        \n        \n    def decode_and_resize(self, imagePath):\n        image = tf.io.read_file(imagePath)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        image = tf.image.resize(image, self.imageSize)\n        return image\n    \n    \n    def __call__(self, anchor, positive, negative):\n        anchor = self.decode_and_resize(anchor)\n        positive = self.decode_and_resize(positive)\n        negative = self.decode_and_resize(negative)\n        return (anchor, positive, negative)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.951270Z","iopub.execute_input":"2024-03-20T11:58:52.951545Z","iopub.status.idle":"2024-03-20T11:58:52.960653Z","shell.execute_reply.started":"2024-03-20T11:58:52.951521Z","shell.execute_reply":"2024-03-20T11:58:52.959785Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class TripletGenerator:\n    def __init__(self, datasetPath):\n        self.peopleNames = list()\n        for folderName in os.listdir(datasetPath):\n            absoluteFolderName = os.path.join(datasetPath, folderName)\n            numImages = len(os.listdir(absoluteFolderName))\n            if numImages > 1:\n                self.peopleNames.append(absoluteFolderName)\n        self.allPeople = self.generate_all_people_dict()\n        \n        \n    def generate_all_people_dict(self):\n        allPeople = dict()\n        for personName in self.peopleNames:\n            imageNames = os.listdir(personName)\n            personPhotos = [\n                os.path.join(personName, imageName) for imageName in imageNames\n            ]\n            allPeople[personName] = personPhotos\n        return allPeople\n    \n    \n    def get_next_element(self):\n        while True:\n            anchorName = random.choice(self.peopleNames)\n            temporaryNames = self.peopleNames.copy()\n            temporaryNames.remove(anchorName)\n            negativeName = random.choice(temporaryNames)\n            (anchorPhoto, positivePhoto) = np.random.choice(\n                a=self.allPeople[anchorName],\n                size=2,\n                replace=False\n            )\n            negativePhoto = random.choice(self.allPeople[negativeName])\n            yield (anchorPhoto, positivePhoto, negativePhoto)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.963379Z","iopub.execute_input":"2024-03-20T11:58:52.963662Z","iopub.status.idle":"2024-03-20T11:58:52.975059Z","shell.execute_reply.started":"2024-03-20T11:58:52.963635Z","shell.execute_reply":"2024-03-20T11:58:52.974191Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"imageSize = (240,240,3)\n\ndef get_siamese_network(imageSize, embeddingModel):\n    anchorInput = keras.Input(name=\"anchor\", shape=imageSize)\n    positiveInput = keras.Input(name=\"positive\", shape=imageSize)\n    negativeInput = keras.Input(name=\"negative\", shape=imageSize)\n    anchorEmbedding = embeddingModel(anchorInput)\n    positiveEmbedding = embeddingModel(positiveInput)\n    negativeEmbedding = embeddingModel(negativeInput)\n    \n    siamese_network = keras.Model(\n        inputs=[anchorInput, positiveInput, negativeInput],\n        outputs=[anchorEmbedding, positiveEmbedding, negativeEmbedding]\n    )\n    \n    return siamese_network","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.976117Z","iopub.execute_input":"2024-03-20T11:58:52.976398Z","iopub.status.idle":"2024-03-20T11:58:52.989011Z","shell.execute_reply.started":"2024-03-20T11:58:52.976374Z","shell.execute_reply":"2024-03-20T11:58:52.988141Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class SiameseModel(keras.Model):\n    def __init__(self, siameseNetwork, margin, lossTracker):\n        super().__init__()\n        self.siameseNetwork = siameseNetwork\n        self.margin = margin\n        self.lossTracker = lossTracker\n        \n    def _compute_distance(self, inputs):\n        (anchor, positive, negative) = inputs\n        embeddings = self.siameseNetwork((anchor, positive, negative))\n        anchorEmbedding = embeddings[0]\n        positiveEmbedding = embeddings[1]\n        negativeEmbedding = embeddings[2]\n        apDistance = tf.reduce_sum(\n            tf.square(anchorEmbedding - positiveEmbedding), axis=-1\n        )\n        anDistance = tf.reduce_sum(\n            tf.square(anchorEmbedding - negativeEmbedding), axis=-1\n        )\n        return (apDistance, anDistance)\n    \n    \n    def _compute_loss(self, apDistance, anDistance):\n        loss = apDistance - anDistance\n        loss = tf.maximum(loss + self.margin, 0.0)\n        return loss\n    \n    \n    def call(self, inputs):\n        (apDistance, anDistance) = self._compute_distance(inputs)\n        return (apDistance, anDistance)\n    \n    \n    def train_step(self, inputs):\n        with tf.GradientTape() as tape:\n            (apDistance, anDistance) = self._compute_distance(inputs)\n            loss = self._compute_loss(apDistance, anDistance)\n        gradients = tape.gradient(\n            loss,\n            self.siameseNetwork.trainable_variables)\n        self.optimizer.apply_gradients(\n            zip(gradients, self.siameseNetwork.trainable_variables)\n        )\n        self.lossTracker.update_state(loss)\n        return {\"loss\": self.lossTracker.result()}\n    \n    \n    def test_step(self, inputs):\n        (apDistance, anDistance) = self._compute_distance(inputs)\n        loss = self._compute_loss(apDistance, anDistance)\n        self.lossTracker.update_state(loss)\n        return {\"loss\": self.lossTracker.result()}\n    \n    \n    @property\n    def metrics(self):\n        return [self.lossTracker]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:52.990364Z","iopub.execute_input":"2024-03-20T11:58:52.990621Z","iopub.status.idle":"2024-03-20T11:58:53.003820Z","shell.execute_reply.started":"2024-03-20T11:58:52.990600Z","shell.execute_reply":"2024-03-20T11:58:53.003005Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainTripletGenerator = TripletGenerator(datasetPath='/kaggle/input/rfcdata/reflection-connection-train-data/train')\nvalTripletGenerator = TripletGenerator(datasetPath='/kaggle/input/rfcdata/reflection-connection-train-data/test')","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:53.004775Z","iopub.execute_input":"2024-03-20T11:58:53.005027Z","iopub.status.idle":"2024-03-20T11:58:53.037520Z","shell.execute_reply.started":"2024-03-20T11:58:53.005006Z","shell.execute_reply":"2024-03-20T11:58:53.036768Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainTfDataset = tf.data.Dataset.from_generator(\n    generator=trainTripletGenerator.get_next_element,\n    output_signature=(\n        tf.TensorSpec(shape=(), dtype=tf.string),\n        tf.TensorSpec(shape=(), dtype=tf.string),\n        tf.TensorSpec(shape=(), dtype=tf.string),\n    )\n)\n\nvalTfDataset = tf.data.Dataset.from_generator(\n    generator=valTripletGenerator.get_next_element,\n    output_signature=(\n        tf.TensorSpec(shape=(), dtype=tf.string),\n        tf.TensorSpec(shape=(), dtype=tf.string),\n        tf.TensorSpec(shape=(), dtype=tf.string),\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:53.038505Z","iopub.execute_input":"2024-03-20T11:58:53.038804Z","iopub.status.idle":"2024-03-20T11:58:53.096284Z","shell.execute_reply.started":"2024-03-20T11:58:53.038780Z","shell.execute_reply":"2024-03-20T11:58:53.095477Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"mapFunction = MapFunction(imageSize=(240,240))\ntrainDs = (trainTfDataset\n    .map(mapFunction)\n    .shuffle(64)\n    .batch(32)\n    .prefetch(tf.data.AUTOTUNE)\n)\nvalDs = (valTfDataset\n    .map(mapFunction)\n    .batch(32)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\nsiameseNetwork =  get_siamese_network(\n    imageSize = (240,240,3),\n    embeddingModel=embeddings,\n)\n\nsiameseModel = SiameseModel(\n    siameseNetwork=siameseNetwork,\n    margin=0.5,\n    lossTracker=keras.metrics.Mean(name=\"loss\"),\n)\n\nsiameseModel.compile(\n    optimizer=keras.optimizers.Adam(0.0001)\n)\n\nsiameseModel.fit(\n    trainDs,\n    steps_per_epoch=50,\n    validation_data=valDs,\n    validation_steps=10,\n    epochs=35,\n)\n\nif not os.path.exists('/kaggle/working/'):\n    os.makedirs('/kaggle/working/')\n\nmodelPath = '/kaggle/working/'\n\nkeras.models.save_model(\n    model=siameseModel.siameseNetwork,\n    filepath=modelPath,\n    include_optimizer=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:58:59.674415Z","iopub.execute_input":"2024-03-20T11:58:59.674806Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/35\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1710935996.141696     252 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"50/50 [==============================] - 120s 848ms/step - loss: 0.4969 - val_loss: 0.4733\nEpoch 2/35\n50/50 [==============================] - 36s 715ms/step - loss: 0.4874 - val_loss: 0.4590\nEpoch 3/35\n50/50 [==============================] - 36s 713ms/step - loss: 0.4890 - val_loss: 0.4988\nEpoch 4/35\n50/50 [==============================] - 36s 713ms/step - loss: 0.4832 - val_loss: 0.4896\nEpoch 5/35\n50/50 [==============================] - 36s 713ms/step - loss: 0.4642 - val_loss: 0.4174\nEpoch 6/35\n50/50 [==============================] - 36s 713ms/step - loss: 0.4585 - val_loss: 0.4532\nEpoch 7/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4350 - val_loss: 0.4536\nEpoch 8/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4342 - val_loss: 0.4359\nEpoch 9/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4376 - val_loss: 0.4108\nEpoch 10/35\n50/50 [==============================] - 36s 711ms/step - loss: 0.4397 - val_loss: 0.4545\nEpoch 11/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4275 - val_loss: 0.4730\nEpoch 12/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4264 - val_loss: 0.4288\nEpoch 13/35\n50/50 [==============================] - 36s 713ms/step - loss: 0.4145 - val_loss: 0.4513\nEpoch 14/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4213 - val_loss: 0.4206\nEpoch 15/35\n50/50 [==============================] - 36s 711ms/step - loss: 0.4130 - val_loss: 0.4725\nEpoch 16/35\n50/50 [==============================] - 36s 711ms/step - loss: 0.3889 - val_loss: 0.5758\nEpoch 17/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4186 - val_loss: 0.4984\nEpoch 18/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4546 - val_loss: 0.4589\nEpoch 19/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4355 - val_loss: 0.4374\nEpoch 20/35\n50/50 [==============================] - 36s 711ms/step - loss: 0.4327 - val_loss: 0.4334\nEpoch 21/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.4168 - val_loss: 0.4403\nEpoch 22/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.3898 - val_loss: 0.4640\nEpoch 23/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.3692 - val_loss: 0.4949\nEpoch 24/35\n50/50 [==============================] - 36s 711ms/step - loss: 0.3703 - val_loss: 0.5097\nEpoch 25/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.3609 - val_loss: 0.5025\nEpoch 26/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.3544 - val_loss: 0.4957\nEpoch 27/35\n50/50 [==============================] - 36s 711ms/step - loss: 0.3431 - val_loss: 0.4578\nEpoch 28/35\n50/50 [==============================] - 36s 712ms/step - loss: 0.3204 - val_loss: 0.5085\nEpoch 29/35\n14/50 [=======>......................] - ETA: 24s - loss: 0.3248","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nfinal_model = load_model(\"/kaggle/working/siamese_CL.h5\", compile=False)\nfinal_model.compile(optimizer='adam', loss=contrastive_loss)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:39:40.541891Z","iopub.execute_input":"2024-03-19T20:39:40.542779Z","iopub.status.idle":"2024-03-19T20:39:46.142795Z","shell.execute_reply.started":"2024-03-19T20:39:40.542734Z","shell.execute_reply":"2024-03-19T20:39:46.141949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport numpy as np\n\nclass ResultBuilder:\n    def __init__(self):\n        self.results = dict()\n        \n    def build(self, \n              query_image_labels: np.ndarray, \n              matched_labels: np.ndarray,   \n              confidence_scores: np.ndarray):\n    \n        if len(query_image_labels.shape) != 1:\n            raise ValueError(f'Expected query_image_labels to be 1-dimensional array, got {query_image_labels.shape} instead')\n        \n        if matched_labels.shape != (query_image_labels.shape[0],3):\n            raise ValueError(f'Expected matched_labels to have shape {(query_image_labels.shape[0], 3)}, got {matched_labels.shape} instead')\n        \n        if confidence_scores.shape != (query_image_labels.shape[0],3):\n            raise ValueError(f'Expected confidence_scores to have shape {(query_image_labels.shape[0], 3)}, got {confidence_scores.shape} instead')\n            \n        for i, x in enumerate(query_image_labels):\n            labels = matched_labels[i]\n            confidence = confidence_scores[i]\n    \n            result_x = [{'label': labels[j], 'confidence': confidence[j]} for j in range(0,3)]\n    \n            self.results.update({x: result_x})\n        \n        return self\n    \n    def to_json(self, path: str = '.') -> None:\n        path = f'{path}/siamese_CL1.json'\n        with open(path, 'w+') as f:\n            json.dump(self.results, f)\n    \n    def __call__(self,\n              query_image_labels: np.ndarray, \n              matched_labels: np.ndarray,   \n              confidence_scores: np.ndarray,\n              path: str = '.') -> None:\n    \n        self.build(query_image_labels, matched_labels, confidence_scores)\n        self.to_json(path)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:40:03.59852Z","iopub.execute_input":"2024-03-19T20:40:03.599046Z","iopub.status.idle":"2024-03-19T20:40:03.610086Z","shell.execute_reply.started":"2024-03-19T20:40:03.599011Z","shell.execute_reply":"2024-03-19T20:40:03.609061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(image_path):\n    image = Image.open(image_path).convert('RGB')\n    width, height = image.size\n    pad = 240\n    image = image.resize((pad, pad))\n    image = image.filter(ImageFilter.SHARPEN)\n    image = np.array(image)\n    image = np.expand_dims(image, axis=0)\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:40:09.117981Z","iopub.execute_input":"2024-03-19T20:40:09.118685Z","iopub.status.idle":"2024-03-19T20:40:09.123865Z","shell.execute_reply.started":"2024-03-19T20:40:09.118656Z","shell.execute_reply":"2024-03-19T20:40:09.123021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_corpus = os.listdir(\"/kaggle/input/rfcdata/reflection-connection-test-data/image_corpus\")\ncorpus = []\n\nfor i in image_corpus:\n    x = preprocess(\"/kaggle/input/rfcdata/reflection-connection-test-data/image_corpus/\"+i)\n    corpus.append(x)\n    \nquery = os.listdir(\"/kaggle/input/rfcdata/reflection-connection-test-data/query\")\nqueries= []\n\nfor i in query:\n    x = preprocess(\"/kaggle/input/rfcdata/reflection-connection-test-data/query/\"+i)\n    queries.append(x)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:40:12.873824Z","iopub.execute_input":"2024-03-19T20:40:12.874655Z","iopub.status.idle":"2024-03-19T20:40:16.221652Z","shell.execute_reply.started":"2024-03-19T20:40:12.874624Z","shell.execute_reply":"2024-03-19T20:40:16.220835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 21\nq = queries[index]\nsims = []\nfor i in range(len(corpus)):\n    sim = model.predict([q, corpus[i]], verbose= 0)\n    sim=sim[0][0]\n    sims.append((sim,i))\n    \nsims = sorted(sims)\nsims","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:42:22.003777Z","iopub.execute_input":"2024-03-19T20:42:22.004547Z","iopub.status.idle":"2024-03-19T20:42:51.55987Z","shell.execute_reply.started":"2024-03-19T20:42:22.004513Z","shell.execute_reply":"2024-03-19T20:42:51.558832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sims[:3]","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:43:26.212377Z","iopub.execute_input":"2024-03-19T20:43:26.212739Z","iopub.status.idle":"2024-03-19T20:43:26.21894Z","shell.execute_reply.started":"2024-03-19T20:43:26.212712Z","shell.execute_reply":"2024-03-19T20:43:26.21808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def euclidean_to_similarity(euclidean_distance, scale_factor=1.0):\n    return np.exp(-scale_factor * euclidean_distance)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:42:51.561844Z","iopub.execute_input":"2024-03-19T20:42:51.562214Z","iopub.status.idle":"2024-03-19T20:42:51.567273Z","shell.execute_reply.started":"2024-03-19T20:42:51.56218Z","shell.execute_reply":"2024-03-19T20:42:51.566317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_sims = []\nfor i in sims[:3]:\n    similarity = euclidean_to_similarity(i[0])\n    new_sims.append((similarity,i[1]))\nnew_sims","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:45:05.671231Z","iopub.execute_input":"2024-03-19T20:45:05.671599Z","iopub.status.idle":"2024-03-19T20:45:05.67946Z","shell.execute_reply.started":"2024-03-19T20:45:05.671571Z","shell.execute_reply":"2024-03-19T20:45:05.678581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matched_labels = []\nconfidence_scores = []\n\nfor i in range(len(queries)):\n    q = queries[i]\n    sims = []\n    for j in range(len(corpus)):\n        sim = model.predict([q, corpus[j]], verbose= 0)[0][0]\n        sims.append((sim,j))\n    sims = sorted(sims)\n    new_sims = []\n    for i in sims[:3]:\n        similarity = euclidean_to_similarity(i[0])\n        new_sims.append((similarity,i[1]))\n        \n    a = []\n    b = []\n    for entry in new_sims:\n        a.append(image_corpus[entry[1]])\n        b.append(float(entry[0]))\n        \n    matched_labels.append(a)\n    confidence_scores.append(b)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T20:45:53.048997Z","iopub.execute_input":"2024-03-19T20:45:53.049384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_image_labels = np.array(query)\nmatched_labels = np.array(matched_labels)\nconfidence_scores = np.array(confidence_scores)\n\nresult_builder = ResultBuilder()\nresult_builder.build(query_image_labels, matched_labels, confidence_scores)\nresult_builder.to_json(\"/kaggle/working/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}